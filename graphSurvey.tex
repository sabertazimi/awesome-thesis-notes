% !TEX program = xelatex
% !BIB program = bibtex

\documentclass[UTF8,12pt,a4paper]{article}
\usepackage{ctex}

% layout
\usepackage[left=2.5cm,right=2.5cm]{geometry}
\usepackage{paralist}     % for compactitem environment
\usepackage{indentfirst}  % ident the first paragraph
\linespread{1.25}
% \makeatletter
% \def\@seccntformat#1{%
%   \expandafter\ifx\csname c@#1\endcsname\c@section
%   Section \thesection:
%   \else
%   \csname the#1\endcsname\quad
%   \fi}
% \makeatother
 
% page headings
\usepackage{fancyhdr}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}
\lhead{\leftmark}
\rhead{M201873026 Yilong Liu}
\cfoot{\thepage}
% \makeatletter
% \let\headauthor\@author
% \makeatother

% url/ref
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black,
  pdfauthor={Yilong Liu},
  pdftitle={Graph Processing: state-of-the-art and research challenges}
}

% vertical centering title page
\usepackage{titling}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}

% table of contents
\usepackage{tocloft}
\renewcommand\cftsecfont{\normalfont}
\renewcommand\cftsecpagefont{\normalfont}
\renewcommand{\cftsecleader}{\cftdotfill{\cftsecdotsep}}
\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}
\renewcommand\cftsubsubsecdotsep{\cftdot}
\renewcommand{\contentsname}{\hfill\bfseries\Large Contents\hfill}   
\setlength{\cftbeforesecskip}{10pt}

% figures
\usepackage{graphicx}
\graphicspath{figures/}
% \newcommand\figureht{\dimexpr
%   \textheight-3\baselineskip-\parskip-.2em-
%   \abovecaptionskip-\belowcaptionskip\relax}


% tables
\usepackage{caption} 
% \captionsetup[table]{skip=10pt}

% math, algorithms, code
\usepackage{amsmath,amssymb,url}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{listings}

\lstset{
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}

% bibliography
\usepackage[super,square,comma,sort]{natbib} % for \citet and \citep
\renewcommand{\refname}{References}
% \begin{filecontents}{report.bib}
% \end{filecontents} 

% appendix
\usepackage{appendix}

\title{Survey \\ \bigskip \textbf{Graph Processing: state-of-the-art and research challenges}}
\author{Huazhong University of Science and Technology\\ School of Computer Science and Technology\\ M1801\\ M201873026\\ Yilong Liu}
\date{\today}

\begin{document}

\pagenumbering{gobble} % no page number
\maketitle
\newpage
% \null\thispagestyle{empty}
% \newpage

% \pagenumbering{roman}
% \section*{Abstract}\sectionmark{Abstract}
% \addcontentsline{toc}{section}{Abstract}
% \addcontentsline{toc}{section}{\protect\numberline{}Abstract}
% \newpage
% \pagenumbering{gobble} % no page number

\pagenumbering{roman}
\tableofcontents
\newpage
% \null\thispagestyle{empty}
% \newpage

\pagenumbering{arabic}

\section{Workload Characterization}
\begin{compactitem}
  \item Memory bandwidth is not fully utilized.
  there is the potential for significant performance improvement
  on graph codes with current off-chip memory systems.
  \item Graph codes exhibit substantial locality.
  Optimized graph codes experience a moderately
  high last-level cache (LLC) hit rate.
  \item Reorder buffer size limits achievable memory throughput.
  The relatively high LLC hit rate implies
  many instructions are executed for each LLC miss.
  These instructions fill the reorder buffer in the core,
  preventing future loads that will miss in the LLC from issuing early,
  resulting in unused memory bandwidth.
  \item Multithreading has limited potential for graph processing
  Likely achievable performance with only a modest number (2) of threads per core.
\end{compactitem}

Because message-passing is far less efficient than accessing memory in contemporary systems,
the efficiency of each core in a cluster is on average
one to two orders-of-magnitude lower than cores in shared-memory nodes.
This communication-bound behavior has led to
a single SSD Node is able to outperform a medium-sized cluster~\cite{DBLP:conf/osdi/KyrolaBG12}.

Graph algorithms have their scaling hampered by
load imbalance, synchronization overheads, and non-uniform memory access (NUMA) penalties.
Different input graph sizes and topologies can lead to
very different conclusions for algorithms and architectures

\subsection{Graph Background}
Graphs can be divided into two broad categories: meshes and social networks.
Meshes (such as road maps or the finiteelement mesh of a simulated car body)
usually have a high diameter
and a degree distribution that is both bounded and low.
social networks have a low diameter (``small-world'')
and a power-law degree distribution (``scale-free'').

In a small-world graph, most nodes are not neighbors of one another,
but most nodes can be reached from every other by a small number of hops.
A scalefree graph has a degree distribution
that follows a power law, at least asymptotically.
The small-world property makes them difficult to partition,
the scale-free property difficult to load balance a parallel execution.

Uniform graph is low diameter, like a social network,
but its degree distribution is normal rather than a power law.
Uniform represents the most adversarial graph,
as by design it has no locality,
it serves to act as lower bound on performance.
\subsection{Graph Benchmark}
No executions sustain a high IPC and a high memory bandwidth
(MLP, memory-level parallelism).
A processor can only execute instructions at a high rate
if it rarely waits on memory, and hence consumes little memory bandwidth.
Some executions are actually in the worst lower-left quadrant,
where they use little memory bandwidth, but their compute throughput is also low,
presumably due to \textbf{memory latency}.

TLB misses are only measurably detrimental when
at least a moderate amount of memory bandwidth is utilized.

If the processor is already achieving moderate memory bandwidth utilization,
performance is insensitive to the branch misprediction rate.
When the processor is not memory-bound,
frequent branch mispredictions will hurt performance.

Lower cache misses, lower memory bandwidth.
Higher degress and scale-free graphs has lower cache misses.

With more cores, this NUMA penalty is reduced,
and for executions that use less memory bandwidth,
the NUMA penalty is reduced further.
Moving computation is better than moving data
when optimizing graph processing for NUMA.

Multithreading also has the potential to introduce new performance challenges.
More threads increase parallelism, which in turn can worsen the damage
caused by load imbalances and synchronization overheads.

Many techniques can improve performance,
but all of them will have quickly diminishing returns,
so greatly improving performance will require a multifaceted approach.

\subsubsection{Benchmark Tips}
\begin{compactitem}
  \item To ensure consistency across runs,
  disable Turbo Boost (dynamic voltage and frequency scaling)
  \item To generate more MLP,
  add more parallel pointer chases to the same thread
\end{compactitem}

\clearpage

\section{Graph Processing System}
图计算框架核心问题:
\begin{compactitem}
  \item 任务划分与映射机制: TLP/DLP + mapping (programming/compiler/runtime)
  \item 独立/统一内存地址空间
  \item 内存数据组织: 图数据结构(CSR/Grid)/NUMA 数据放置
  \item 并发控制与同步机制: BSP/Pull/Push/Dataflow
\end{compactitem}

图计算特点:
\begin{compactitem}
  \item 规模巨大
  \item 局部性差
  \item 耦合性强
  \item 数据可能动态变化
  \item 计算过程频繁迭代
\end{compactitem}

控制流体系结构为主的现代计算机系统带来了访存局部性差, 并行流水执行效率低,
内外存通道有限以及同步的扩展性差等一系列挑战.
随着内存性能的逐渐提高和各种新型内存技术的出现,
图计算本身的调度和同步开销将逐渐成为新的主要性能瓶颈.
由于受到传统控制流体系结构的限制,
现有方案在优化手段上较为单一而难以进一步提高图计算系统的性能.
综上所述，为实现基于异构系统的图计算的``高性能, 可扩展, 易编程''目标,
学术界和工业界还需要在异构平台图计算协同调度, 高效的数据管理机制,
高度抽象且易用的异构图编程框架等关键技术方面进行进一步的研究和突破.
\subsection{图计算系统分类}
按架构分类:
\begin{compactitem}
  \item 单机图计算系统: 指在单机上利用多核CPU, 大内存和多线程并行等进行大规模图计算.
  这类系统通过减少磁盘随机写操作, 避免高昂的通信开销,
  采用并行化技术来充分挖掘多核计算资源来处理大规模图数据,
  在图的规模不是很大的情况下, 能在可接受的时间范围内完成任务,
  达到与分布式大规模图计算系统相当的时间性能.
  已有的主要单机图计算系统包括: GraphChi~\cite{DBLP:conf/osdi/KyrolaBG12},
  X-Stream~\cite{DBLP:conf/sosp/RoyMZ13},
  Ligra~\cite{DBLP:conf/ppopp/ShunB13},
  TurboGraph~\cite{DBLP:conf/kdd/HanLPL0KY13},
  FlashGraph~\cite{DBLP:conf/fast/ZhengMBVPS15},
  GridGraph~\cite{DBLP:conf/usenix/ZhuHC15},
  PathGraph~\cite{DBLP:journals/tpds/YuanXLJ16}.
\end{compactitem}


\subsection{Ligra}
Ligra~\cite{DBLP:conf/ppopp/ShunB13} is built on
the abstractions of edge maps and vertex maps.
When applying these map functions,
Ligra uses heuristics to determine in
which direction to apply them (push or pull)
and what data structures to use (sparse or dense).
These optimizations make Ligra especially well suited for low-diameter graphs.
\subsection{Galois}
Galois~\cite{DBLP:conf/sosp/NguyenLP13}
are designed to handle irregular fine-grained task parallelism.
Algorithms implemented in Galois
are free to use autonomous scheduling (no synchronization barriers),
which reduce the synchronization needed for high-diameter graphs.
\clearpage

\section{FPGA}
FPGA in Data Center~\cite{DBLP:conf/isca/PutnamCCCCDEFGGHHHHKLLPPSTXB14}.

\begin{compactitem}
  \item cpu 通用
  \item gpu 高吞吐量
  \item fpga 低延迟
\end{compactitem}

FPGA 比 CPU 和 GPU 能效高，体系结构上的根本优势是无指令、无需共享内存.
我们发现通过 OpenCL 写 DRAM、启动 kernel、读 DRAM 一个来回，需要 1.8 毫秒。而通过 PCIe DMA 来通信，却只要 1~2 微秒.
在数据中心里 FPGA 的主要优势是稳定又极低的延迟，适用于流式的计算密集型任务和通信密集型任务.
FPGA 定义为通信的「大管家」，不管是服务器跟服务器之间的通信，虚拟机跟虚拟机之间的通信，进程跟进程之间的通信，CPU 跟存储设备之间的通信，都可以用 FPGA 来加速.
不管通信还是机器学习、加密解密，算法都是很复杂的,
如果试图用 FPGA 完全取代 CPU，势必会带来 FPGA 逻辑资源极大的浪费，也会提高 FPGA 程序的开发成本.
更实用的做法是 FPGA 和 CPU 协同工作，局部性和重复性强的归 FPGA，复杂的归 CPU.
\clearpage

\section{DRAM}
Core (核) 代表一个独立的 CPU, Socket (颗) 本意是插槽,
代表一个主板上的芯片, 多核架构就是在一个芯片上放多个处理器,
2颗4核, 代表主板上有 2 个芯片插槽, 每个芯片上存在 4 个 CPU, 所以有 8 个物理 CPU,
如果存在超线程, 还可能多出 8 个逻辑 CPU, 总共 16 个逻辑 CPU.
一个逻辑 CPU（LCPU）能运行一个进程或线程, 如果每核能执行 2 个或更多的线程,
那么一定使用了超线程技术, 否则每一核就只能运行一个线程或进程.

SRAM 容量太难做大, CPU 一半以上的面积都用来做 SRAM.
SRAM 面积越大, 速度就越慢.
SRAM 做到 GB 容量几乎是不可能完成的任务.
\subsection{Basis of DRAM}
\subsubsection{Terminology}
\begin{compactitem}
  \item RAS: Row Address Strobe (ACT Activate DRAM page/row Command)
  \item CAS: Column Address Strobe (READ Command)
  \item DDR: Double-Data Rate transfers data
  on both rising and falling edge of the clock
  \item Channel: 一组数据信号线、对应几个槽位、对应几根内存条称为一个 channel
  \item Bank: read out all words in parallel, 一个 bank 包含 N 个 DRAM Subarray (N*row*col*1bit)
  \item DIMM: Dual Inline Memory Module 双通道内存
  \item channel＞DIMM＞rank＞chip＞bank＞row/column
\end{compactitem}
\subsubsection{Address Mapping Scheme}
如表~\ref{tab:dram_address_mapping}所示,
$\text{Memory Capacity} = K*L*B*R*C*V$,
$N = CV/Z$, $CV = NZ$.
采用 open page (keep page in DRAM row buffer) 时,
映射为 k:l:r:b:n:z (可扩展) /r:l:b:n:k:z (高并行) (n 也等于 column, z 也等于 offset).
采用 close page (immediately close page in DRAM row buffer) 时,
映射为 k:l:r:n:b:z/r:n:l:b:k:z.
Accessing different rows from one bank is slowest.
\begin{table}
  \begin{small}
    \caption{DRAM Address Mapping Parameters}
    \label{tab:dram_address_mapping}
    \begin{center}
      \begin{tabular}[c]{l|l}
        \hline
        \multicolumn{1}{c|}{\textbf{Symbol}} & 
        \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        K & \# of channels in system \\
        L & \# of ranks per channel \\
        B & \# of banks per rank \\
        R & \# of rows per bank \\
        C & \# of columns per row \\
        V & \# of bytes per column \\
        Z & \# of bytes per cache line \\
        N & \# of cache lines per row \\
        \hline
      \end{tabular}
    \end{center}
  \end{small}
\end{table}
\subsubsection{Delay Time}
一条访存指令发到内存控制器, 它的访存延时是存在不同的可能性:
\begin{compactitem}
  \item row buffer hit: 从 row buffer 到把数据放在数据总线上的时延，大约 20 ns
  \item empty row buffer: 从电容到 sense amplifier 再到 row buffer 的时序 + 从 row buffer 到数据总线时间，大约40ns
  \item row buffer conflict: 写回时延 + empty row buffer delay, 大约60ns
  \item CL: CAS Latency, 从 CAS 与读取命令发出到第一笔数据输出的这段时间 (READ -> data)
  \item tRCD: RAS 到 CAS 时延 (Active -> READ)
  \item 要切换另一行，要发 precharge 命令 (close page/row), 把数据写到 cell 里去.
  关闭一个行需要时间, 这个时间称为 tRP, 发送 PRE/PREA 命令后 tRP 时间才可以发 ACT 命令.
\end{compactitem}
\subsection{Memory Controller}
来自 CPU 的请求以执行顺序缓冲进入 transaction queue,
这些请求被转换为 DRAM 命令并放入 command queue.
\subsection{Landscape of DRAM-based memory}
\begin{table}
  \begin{small}
    \caption{Landspace of DRAM-based memory~\cite{DBLP:journals/cal/KimYM16}}
    \label{tab:memory_landscape}
    \begin{center}
      \begin{tabular}[c]{l|l}
        \hline
        \multicolumn{1}{c|}{\textbf{Segment}} & 
        \multicolumn{1}{c}{\textbf{DRAM Standards \& Architectures}} \\
        \hline
        Commodity & DDR3;DDR4 \\
        Low-Power & LPDDR3;LPDDR4 \\
        Graphics & GDDR5 \\
        Performance & eDRAM;RLDRAM3;WIO;WIO2;MCDRAM \\
        3D-Stakced & HBM;HMC;SBA/SSA;Staged Reads;RAIDR \\
        Academic & SALP/SARP;AL/TL-DRAM;RowClone;Half-DRAM;Row-Buffer Decoupling \\
        \hline
      \end{tabular}
    \end{center}
  \end{small}
\end{table}
\subsection{Simulator}
\subsubsection{DRAMSim2}
\begin{compactitem}
  \item AddressMapping.cpp: 物理地址到内存 (rank/bank/row/col/offset) 的映射关系
  \item TraceBasedSim.cpp: trace file (.trc) 相关代码
\end{compactitem}
\clearpage

\section{Graph Survey Template}
\begin{compactitem}
  \item data presentation
  \item data source
  \item data manipulation
  \item open source
  \item programming model
  \item partioning
  \item static and dynamic graphs
  \item batch and streaming
  \item algorithms
  \item community activity
  \item users
  \item books 
  \item distinction points
\end{compactitem}
\clearpage

\bibliographystyle{unsrt}
\bibliography{bibs/graph}
\addcontentsline{toc}{section}{References}
\newpage

\end{document}
